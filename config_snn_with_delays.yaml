################################################
#                    Delays                    #
################################################

DCLSversion: 'gauss'  

learn_delay: false  # Whether to enable learning of delays in the model

max_delay: 11  # Maximum delay in time steps, adjusted to ensure it's odd

# Initialization of positional delay parameters
#init_pos_method: 'middle' 
#init_pos_method: 'right' 
init_pos_method: 'uniform'  # Method to initialize delay positions 
init_pos_a: -5 # Lower bound for uniform distribution of positions
init_pos_b: 5  # Upper bound for uniform distribution of positions

decrease_sig_method: 'exp' 
kernel_count: 1  

################################################
#            Common configuration             #
################################################

seed: 0  # Seed for random number generation

time_step: 16  # Time step for simulation in ms
batch_size: 4  # Batch size for training

n_hidden_layers: 5  # Number of hidden layers in the neural network
n_hidden_neurons: 128  # Number of neurons in each hidden layer

# data related parameters
n_inputs: 2  # Number of input features
n_outputs: 11  # Number of output classes
height: 128  # Height of the input images or feature map
width: 128  # Width of the input images or feature map

init_tau: 2.0   # Initial membrane time constant (tau) (*time_step)

use_maxpool: true  # Whether to use max pooling in the network
use_batchnorm: true  # Whether to use batch normalization
bias: false  # Whether to include bias terms in the model
detach_reset: true  # Whether to detach the reset from backpropagation (for stability)

output_v_threshold: 1_000_000  # Voltage threshold for output neurons (extremely high value)

v_threshold: 1.0  # Threshold for neuron voltage to trigger a spike
alpha: 2.0  # Alpha value for the surrogate activation function

init_w_method: 'kaiming_uniform'  # Initialization method for weights

################################################
#               Optimizer hyperparameters      #
################################################

lr_w: 0.001  # Learning rate for weight parameters
lr_pos: 0.1  # Learning rate for pos
weight_decay: 0.00001  # Weight decay (L2 regularization)
dropout_p: 0.25  # Dropout probability for regularization

